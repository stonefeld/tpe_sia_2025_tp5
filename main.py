import numpy as np
from sklearn.decomposition import PCA

from src.activators import sigmoid, sigmoid_prime
from src.autoencoders import AutoencoderMLP as Autoencoder
from src.optimizers import Adam
from src.plots import plot_all_letters, plot_error_distribution, plot_latent_space
from src.utils import pixel_error

font_data = [
    [0x04, 0x04, 0x02, 0x00, 0x00, 0x00, 0x00],  # 0x60, `
    [0x00, 0x0E, 0x01, 0x0D, 0x13, 0x13, 0x0D],  # 0x61, a
    [0x10, 0x10, 0x10, 0x1C, 0x12, 0x12, 0x1C],  # 0x62, b
    [0x00, 0x00, 0x00, 0x0E, 0x10, 0x10, 0x0E],  # 0x63, c
    [0x01, 0x01, 0x01, 0x07, 0x09, 0x09, 0x07],  # 0x64, d
    [0x00, 0x00, 0x0E, 0x11, 0x1F, 0x10, 0x0F],  # 0x65, e
    [0x06, 0x09, 0x08, 0x1C, 0x08, 0x08, 0x08],  # 0x66, f
    [0x0E, 0x11, 0x13, 0x0D, 0x01, 0x01, 0x0E],  # 0x67, g
    [0x10, 0x10, 0x10, 0x16, 0x19, 0x11, 0x11],  # 0x68, h
    [0x00, 0x04, 0x00, 0x0C, 0x04, 0x04, 0x0E],  # 0x69, i
    [0x02, 0x00, 0x06, 0x02, 0x02, 0x12, 0x0C],  # 0x6a, j
    [0x10, 0x10, 0x12, 0x14, 0x18, 0x14, 0x12],  # 0x6b, k
    [0x0C, 0x04, 0x04, 0x04, 0x04, 0x04, 0x04],  # 0x6c, l
    [0x00, 0x00, 0x0A, 0x15, 0x15, 0x11, 0x11],  # 0x6d, m
    [0x00, 0x00, 0x16, 0x19, 0x11, 0x11, 0x11],  # 0x6e, n
    [0x00, 0x00, 0x0E, 0x11, 0x11, 0x11, 0x0E],  # 0x6f, o
    [0x00, 0x1C, 0x12, 0x12, 0x1C, 0x10, 0x10],  # 0x70, p
    [0x00, 0x07, 0x09, 0x09, 0x07, 0x01, 0x01],  # 0x71, q
    [0x00, 0x00, 0x16, 0x19, 0x10, 0x10, 0x10],  # 0x72, r
    [0x00, 0x00, 0x0F, 0x10, 0x0E, 0x01, 0x1E],  # 0x73, s
    [0x08, 0x08, 0x1C, 0x08, 0x08, 0x09, 0x06],  # 0x74, t
    [0x00, 0x00, 0x11, 0x11, 0x11, 0x13, 0x0D],  # 0x75, u
    [0x00, 0x00, 0x11, 0x11, 0x11, 0x0A, 0x04],  # 0x76, v
    [0x00, 0x00, 0x11, 0x11, 0x15, 0x15, 0x0A],  # 0x77, w
    [0x00, 0x00, 0x11, 0x0A, 0x04, 0x0A, 0x11],  # 0x78, x
    [0x00, 0x11, 0x11, 0x0F, 0x01, 0x11, 0x0E],  # 0x79, y
    [0x00, 0x00, 0x1F, 0x02, 0x04, 0x08, 0x1F],  # 0x7a, z
    [0x06, 0x08, 0x08, 0x10, 0x08, 0x08, 0x06],  # 0x7b, {
    [0x04, 0x04, 0x04, 0x00, 0x04, 0x04, 0x04],  # 0x7c, |
    [0x0C, 0x02, 0x02, 0x01, 0x02, 0x02, 0x0C],  # 0x7d, }
    [0x08, 0x15, 0x02, 0x00, 0x00, 0x00, 0x00],  # 0x7e, ~
    [0x1F, 0x1F, 0x1F, 0x1F, 0x1F, 0x1F, 0x1F],  # 0x7f, DEL
]


def decode_font(font_data):
    images = []
    for glyph in font_data:
        rows = []
        for value in glyph:
            bin_str = format(value, "05b")
            rows.extend([int(b) for b in bin_str])

        images.append(rows)

    return np.array(images, dtype=np.float32)


def pca_2d(latent_representations):
    pca = PCA(n_components=2)
    return pca.fit_transform(latent_representations)


def main():
    decoded_data = decode_font(font_data)
    plot_all_letters(decoded_data)

    layers = [35, 18, 6, 2, 6, 18, 35]
    # layers = [35, 10, 2, 10, 35]
    adam = Adam(learning_rate=0.001, layers=layers)
    autoencoder = Autoencoder(layers=layers, tita=sigmoid, tita_prime=sigmoid_prime, optimizer=adam)

    autoencoder.train(decoded_data, epochs=100000, batch_size=8, max_pixel_error=1)
    latent_representations = autoencoder.get_latent_representations(decoded_data)

    latent_2d = pca_2d(latent_representations)
    plot_latent_space(latent_2d, decoded_data)

    reconstructed = autoencoder.forward(decoded_data)[-1]
    errors = pixel_error(decoded_data, reconstructed)
    print("\n=============================\nPixel errors for each letter:")
    for i, err in enumerate(errors):
        print(f"Letter {i}: {int(err)} pixels")

    plot_all_letters(reconstructed)
    plot_error_distribution(errors)


if __name__ == "__main__":
    main()
